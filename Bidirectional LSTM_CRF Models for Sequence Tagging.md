# Bidirectional LSTM-CRF Models for Sequence Tagging

用于序列标记的双向LSTM-CRF模型

## Abstract

在本文中，我们提出了多种基于长短期记忆（LSTM）的序列标记模型。 这些模型包括LSTM网络，双向LSTM（BI-LSTM）网络，具有条件随机场（CRF）层的LSTM（LSTM-CRF）和具有CRF层的双向LSTM（BI-LSTM-CRF）。 我们的工作是第一个将双向LSTM CRF（表示为BI-LSTM-CRF）模型应用于NLP基准序列标记数据集。 我们证明，由于具有双向LSTM组件，BI-LSTM-CRF模型可以有效地使用过去和将来的输入功能。 由于具有CRF层，它还可以使用句子级别的标签信息。 BI-LSTM-CRF模型可以在POS，分块和NER数据集上产生最新（或接近）准确性。 另外，与以前的观察相比，它是健壮的并且对词嵌入的依赖性较小。

## Introduction

序列标记包括语音标记（POS），分块。 命名实体识别（NER）是NLP的一项经典任务。 几十年来，它引起了研究的关注。 标记器的输出可用于下行流应用程序。 例如，可以使用经过用户搜索查询训练的命名实体识别器来识别哪些文本跨度是产品，从而触发某些产品广告。 另一个例子是，搜索引擎可以使用此类标签信息来找到相关的网页。

现有的大多数序列标签模型都是线性统计模型，包括隐马尔可夫模型（HMM），最大熵马尔可夫模型（MEMM）（McCallum等，2000）和条件随机场（CRF）（Lafferty等，2001）。 最近提出了基于卷积网络的模型（Collobert等，2011）来解决序列标记问题。 我们将这种模型称为Conv-CRF，因为它由卷积网络和输出上的CRF层组成（原始论文中使用了句子级对数似然（SSL）术语）。 Conv-CRF模型已经产生了有关序列标记任务的有希望的结果。 在语音语言理解社区中，最近提出了基于递归神经网络（Mesnil等，2013； Yao等，2014）和卷积网（Xu and Sarikaya，2013）的模型。 其他相关工作包括（Graves等，2005； Graves等，2013），它提出了一种用于语音识别的双向递归神经网络。

在本文中，我们提出了多种基于神经网络的模型来进行序列标记任务。这些模型包括LSTM网络，双向LSTM网络（BI-LSTM），带有CRF层的LSTM网络（LSTM-CRF）和带有CRF层的双向LSTM网络（BI-LSTM-CRF）。我们的贡献可以总结如下。 1）我们系统地比较了上述模型在NLP标记数据集上的性能； 2）我们的工作是第一个将双向LSTM CRF（表示为BI-LSTM-CRF）模型应用于NLP基准顺序标记数据集。由于采用了双向LSTM组件，该模型可以同时使用过去和将来的输入功能。另外，由于具有CRF层，该模型可以使用句子级别的标签信息。我们的模型可以在POS，分块和NER数据集上产生最先进（或接近）的准确性； 3）我们证明，BI-LSTM-CRF模型是健壮的，并且与以前的观察相比，它对单词嵌入的依赖性较小（Collobert等，2011）。它可以产生准确的标记性能，而无需依靠单词嵌入。

在本文的其余部分安排如下。 第2节介绍了本文中使用的序列标记模型。 第3节显示了培训过程。 第4节报告了实验结果。 第五部分讨论相关研究。 最后，第6节得出结论。

## 

在本节中，我们描述了本文使用的模型：LSTM，BI-LSTM，CRF，LSTM-CRF和BI-LSTM-CRF。

### LSTM Networks

递归神经网络（RNN）已被用于在包括语言模型（Mikolov等，2010； Mikolov等，2011）和语音识别（Graves等，2005）的各种任务上产生令人鼓舞的结果。 ）。 RNN基于历史信息维护内存，这使模型能够预测基于长距离要素的当前输出。

图1显示了RNN结构（Elman，1990），它具有输入层x，隐藏层h和输出层y。 在命名实体标记上下文中，x表示输入要素，y表示标记。 图1说明了一个命名的实体识别系统，其中每个单词都用其他（O）或四种实体类型之一标记：人员（PER），位置（LOC），组织（ORG）和其他（MISC）。 欧盟的判决驳回了德国要求抵制英国羔羊的呼吁。 标记为B-ORG O B-MISC O O O B-MISC O O，其中B-，I-标签指示实体的开始位置和中间位置。

输入层表示时间t处的要素。 它们可以是单词特征，密集矢量特征或稀疏特征的一键编码。 输入图层的尺寸与要素尺寸相同。 输出层表示时间t处标签上的概率分布。 它具有与标签大小相同的尺寸。 与前馈网络相比，RNN引入了先前的隐藏状态和当前的隐藏状态之间的联系（因此也引入了递归层权重参数）。 该循环层旨在存储历史信息。 隐藏层和输出层中的值计算如下：



其中U，W和V是训练时间中要计算的连接权重，而f（z）和g（z）是S型和softmax激活函数，如下所示。

在本文中，我们将长短期记忆（Hochreiter和Schmidhuber，1997； Graves等，2005）应用于序列标记。 长短期存储网络与RNN相同，不同之处在于隐藏层更新被专用存储单元代替。 结果，它们可能会更好地发现和利用数据中的长期依赖关系。 图2说明了一个LSTM存储单元（Graves等，2005）。  LSTM存储单元的实现如下：



其中σ是逻辑S形函数，而i，f，o和c是输入门，忘记门，输出门和像元向量，它们的大小均与隐藏向量h相同。 权重矩阵下标具有其名称所暗示的含义。 例如，Whi是隐藏输入门矩阵，Wxo是输入输出门矩阵等。从单元到门向量的权重矩阵（例如Wci）是对角线，因此每个门向量中的元素m仅 从单元向量的元素m接收输入。

图3示出了采用上述LSTM存储单元（带有圆角的虚线框）的LSTM序列标记模型。

### Bidirectional LSTM Networks

在序列标记任务中，我们可以在给定时间内访问过去和将来的输入功能，因此可以利用双向的LSTM网络（图4）（Graves等，2013）。 这样，我们就可以在特定的时间范围内有效地利用过去的功能（通过正向状态）和将来的功能（通过反向状态）。 我们使用时间反向传播（BPTT）训练双向LSTM网络（Boden。，2002）。 随时间推移，展开网络上的正向和反向传递与常规网络正向和反向传递相似，只是我们需要为所有时间步长展开隐藏状态。 我们还需要在数据点的开头和结尾处进行特殊处理。 在我们的实现中，我们对整个句子进行前进和后退操作，我们只需要在每个句子开始时将隐藏状态重置为0。 我们有批处理实现，它使得可以同时处理多个句子。

### CRF Networks

在预测当前标签时，有两种方法可以利用邻居标签信息。 首先，要预测每个时间步长的标签分布，然后使用类似波束的解码来找到最佳标签序列。 最大熵分类器（Ratnaparkhi，1996）和最大熵马尔可夫模型（MEMM）（McCallum等，2000）的工作属于这一类。 第二个是关注句子级别而不是单个位置，因此导致了条件随机场（CRF）模型（Lafferty等，2001）（图5）。 请注意，与使用存储单元/循环组件的LSTM和双向LSTM网络相反，输入和输出是直接连接的。

已经表明，CRF通常可以产生更高的标记精度。 有趣的是，这两种使用标签信息的方式之间的关系与两种使用输入功能的方式相似（请参见上述LSTM和BI-LSTM网络），并且本文的结果证实了BI-LSTM的优越性 符合LSTM。

### LSTM-CRF Networks





